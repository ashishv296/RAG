{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2m2l-vt_RSp"
      },
      "source": [
        "# Build a RAG system with LLAMA3 'Llama 3B-Instruct' for chatting with your PDF files!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKZWdc1_J5hm"
      },
      "source": [
        "In this quick tutorial, we'll build a simple RAG system with the latest LLM from Meta - Llama 3, specifically the `Llama-3-8B-Instruct` version.\n",
        "\n",
        "Key Components : 'Unstructured API' for preprocessing PDF files, LangChain for RAG, FAISS for vector storage, and HuggingFace `transformers` to get the model. Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKS2grloNhrM"
      },
      "source": [
        "Step-1 : Install all the libraries (e.g.-langchain,FAISS)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ9juQ-XKJKK"
      },
      "outputs": [],
      "source": [
        "!pip install -q unstructured-client unstructured[all-docs] langchain transformers accelerate bitsandbytes sentence-transformers faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-2 : Get your [free unstructured API key](https://unstructured.io/api-key-free), and instantiate the Unstructured client to preprocess your PDF file:"
      ],
      "metadata": {
        "id": "62dnhv_X-Ee9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3JWGSEoZKbtN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"UNSTRUCTURED_API_KEY\"] = \"UNSTRUCTURED_API_KEY-GET AND REPLACE FROM THE ABOVE LINK\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OIG6V3swKyIZ"
      },
      "outputs": [],
      "source": [
        "from unstructured_client import UnstructuredClient\n",
        "\n",
        "unstructured_api_key = os.environ.get(\"UNSTRUCTURED_API_KEY\")\n",
        "\n",
        "client = UnstructuredClient(\n",
        "    api_key_auth=unstructured_api_key,\n",
        "    # if using paid API, provide your unique API URL:\n",
        "    # server_url=“YOUR_API_URL”,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwzrH-9_K6-z"
      },
      "source": [
        "Step-3 : Partition, and chunking of the Custom Knowledge Base (pdf file) with 'Unstructured API' so that the logical structure of the document is preserved for better RAG results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qThmnl_fJzrb"
      },
      "outputs": [],
      "source": [
        "from unstructured_client.models import shared\n",
        "from unstructured_client.models.errors import SDKError\n",
        "from unstructured.staging.base import dict_to_elements\n",
        "\n",
        "path_to_pdf=\"/content/drive/MyDrive/Colab Notebooks/pdf/Retrieval Augmented Generation.pdf\"\n",
        "\n",
        "with open(path_to_pdf, \"rb\") as f:\n",
        "  files=shared.Files(\n",
        "      content=f.read(),\n",
        "      file_name=path_to_pdf,\n",
        "      )\n",
        "  req = shared.PartitionParameters(\n",
        "    files=files,\n",
        "    chunking_strategy=\"by_title\",\n",
        "    max_characters=512,\n",
        "  )\n",
        "  try:\n",
        "    resp = client.general.partition(req)\n",
        "  except SDKError as e:\n",
        "    print(e)\n",
        "\n",
        "elements = dict_to_elements(resp.elements)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-4 : Hugging Face authenticatation Access tokens programmatically authenticate your identity to the [Hugging Face Hub](https://huggingface.co/settings/tokens)"
      ],
      "metadata": {
        "id": "Pjhd-0p8DRXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "rFSnprFRg539"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF1TaeByPEBR"
      },
      "source": [
        "Step-5 :Create LangChain documents from document chunks and their metadata, and ingest those documents into the FAISS vectorstore.\n",
        "\n",
        "Set up the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bT1XTlQLMBX2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "documents = []\n",
        "for element in elements:\n",
        "    metadata = element.metadata.to_dict()\n",
        "    documents.append(Document(page_content=element.text, metadata=metadata))\n",
        "\n",
        "db = FAISS.from_documents(documents, HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\"))\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSHJYpskQKJB"
      },
      "source": [
        "Step-6 :Now, let's finally set up llama 3 to use for text generation in the RAG system.\n",
        "\n",
        "This is a gated model, which means you first need to go to the [model's page](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), log in, review terms and conditions, and request access to it. To use the model in the notebook, you need to log in with your Hugging Face token (get it in your profile's settings)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "A_ZUs8SuFn5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rmCLySVHDngw"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub.hf_api import HfFolder\n",
        "\n",
        "HfFolder.save_token('YOUR_HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vu81HA_b-ei"
      },
      "source": [
        "Step-7 :Quantization- To run this tutorial in the free Colab GPU, we'll need to quantize the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKG6n2JpMtu3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeOvh7Y2cIym"
      },
      "source": [
        "Step-8 :Set up Llama 3 and a simple RAG chain.\n",
        "Make sure to follow the prompt format for best results:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{{ model_answer_1 }}<|eot_id|>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b-8hFNkoM0LC"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=terminators,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "You are an assistant for answering questions about RAG.\n",
        "You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\n",
        "Question: {question}\n",
        "Context: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "llm_chain = prompt | llm | StrOutputParser()\n",
        "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u18Cc6msce5l"
      },
      "source": [
        "Step-9 :Wow! Your RAG is ready to use. Pass a question, the retriver will add relevant context from your document, and Llama3 will generate an answer.\n",
        "Here, my document was a chapter from a book on IPM that stands for \"Integrated Pest Management\".  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "i47dsLeFNVPB",
        "outputId": "513c64f2-fc24-4835-97b7-87b875266554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"According to the text, RAG helps by allowing the LLM to access external sources of information, such as proprietary documents and data or even the internet. This means that the LLM's responses are no longer limited to its internal knowledge, but can draw on a much broader range of information. Additionally, RAG enables the LLM to retrieve relevant information related to a user's query, which it can then use to generate a more accurate and informative response.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "question = \"How does RAG help?\"\n",
        "rag_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are some popular RAG use cases?\"\n",
        "rag_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "2vxjGWzaoyFe",
        "outputId": "6731b979-46dd-4818-87b8-06b88ce19e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What are some popular RAG use cases?\\n\\nAccording to our documentation, there are several commercial applications of RAG, including:\\n\\n* Document Question Answering Systems: This involves using RAG to provide access to proprietary enterprise documents to a large language model (LLM). The LLM can then respond based on the information retrieved from the documents.\\n\\nThese are just a few examples of how RAG can be used. If you're interested in learning more, I'd be happy to help!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are some popular RAG use cases? please give minimum 3 examples\"\n",
        "rag_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "cUJa2PXqpY6t",
        "outputId": "0427b259-a97d-42e9-9035-043153bac843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What a great question!\\n\\nAccording to our documentation, there are several popular RAG use cases. Here are three examples:\\n\\n1. **Document Question Answering Systems**: This use case involves using RAG to provide access to proprietary enterprise documents to a Large Language Model (LLM). The retriever searches for the most relevant documents and provides the information to the LLM.\\n\\n2. **Content Generation**: In this scenario, RAG is used to generate content based on user input or prompts. The model retrieves relevant information from various sources and uses it to generate high-quality content.\\n\\n3. **Chatbots and Virtual Assistants**: RAG can be applied to chatbots and virtual assistants to enable more accurate and informative conversations with users. The model retrieves relevant information from various sources and uses it to respond to user queries.\\n\\nThese are just a few examples of the many potential use cases for RAG. I hope this helps!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \" In RAG Architecture - What are the five high level steps of an RAG enabled system\"\n",
        "rag_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "trOGjAuLqF9C",
        "outputId": "fcbd5363-4ed6-4471-fef5-b48109befd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are the five high-level steps of an RAG-enabled system?\\n\\nAccording to the RAG architecture, the five high-level steps are:\\n\\n1. **Search & Retrieval**: This step involves searching relevant information from the knowledge sources and retrieving the necessary data.\\n\\n2. **Relevant Context**: In this step, the retrieved data is analyzed to identify the relevant context that needs to be considered for the next step.\\n\\n3. **Augmentation**: Here, the relevant context is added to the prompt, depending on the specific use case.\\n\\n4. **Prompt + Context**: This step combines the original prompt with the added context to create a new prompt that takes into account the relevant information.\\n\\n5. **Generation**: Finally, the augmented prompt is used to generate a response or output, which is then returned as the final result.\\n\\nThese five steps form the core of the RAG architecture, enabling systems to efficiently retrieve and utilize relevant information to generate high-quality responses.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the Two pipelines become important in setting up the RAG system\"\n",
        "rag_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "8fr8eaxur_Mw",
        "outputId": "4ac13a79-c78e-4d29-a59b-9f187d498358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided documents, I found that the two pipelines that become important in setting up the RAG system are:\\n\\n1. **Indexing Pipeline**: This pipeline sets up the knowledge source for the RAG system. It involves four primary steps: loading, splitting, embedding, and storing data.\\n\\n2. **RAG Pipeline**: This pipeline involves three main steps:\\n\\t* Search & Retrieval: Searching for relevant information in the knowledge source.\\n\\t* Augmentation: Adding context to the prompt based on the use case.\\n\\t* Generation: Using the augmented prompt to generate a response.\\n\\nThese pipelines work together to enable the RAG system to retrieve relevant information, augment it with context, and generate a response.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the Two pipelines become important in setting up the RAG system\"\n",
        "rag_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "f_c8tjRPCOAX",
        "outputId": "a722ef93-6eed-40e2-ac27-a175ead325a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'According to the documents provided, the two pipelines that become important in setting up the RAG system are:\\n\\n1. **Indexing Pipeline**: This pipeline sets up the knowledge source for the RAG system. It involves four primary steps: Loading, Splitting, Embedding, and Storage of data.\\n\\nThis pipeline creates the knowledge base by indexing the data from various sources, such as vector databases, and makes it available for querying.\\n\\n2. **RAG Pipeline**: This pipeline involves the actual RAG process, which takes the user query at runtime and retrieves the relevant data from the index, then passes that to the model for generation.\\n\\nThe RAG pipeline consists of three main steps: Search & Retrieval, Augmentation, and Generation. These steps involve searching the context from the source (e.g., vector database), adding the context to the prompt depending on the use case, and finally generating the response based on the retrieved information.\\n\\nThese two pipelines work together to enable the RAG system to retrieve'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}